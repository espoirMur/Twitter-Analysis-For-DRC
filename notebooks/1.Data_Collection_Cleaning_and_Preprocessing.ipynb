{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I am going to clean perform the data collection , data cleanning and data preprosessing and save the results in a csv file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be collecting cleanned tweets from a local database , and save them in a pandas dataframe, and from that I will continue with my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from pathlib import Path\n",
    "\n",
    "def get_dot_env_file(ENV):\n",
    "    if ENV:\n",
    "        return Path.cwd().parent.joinpath(\".env.dev\")\n",
    "    return find_dotenv(Path.cwd().parent.joinpath(\".env.dev\"))\n",
    "\n",
    "def read_credentails(file='.env'):\n",
    "    \"\"\"\n",
    "    Return users credentials from the environnement variable\n",
    "    raise a an exception if the credentials are empty\n",
    "\n",
    "    Raises:\n",
    "        ValueError: raise a value error if no credentials was found\n",
    "    \"\"\"\n",
    "    dot_env_file = get_dot_env_file(file)\n",
    "    load_dotenv(dotenv_path=dot_env_file)\n",
    "    DATABASE_HOST = getenv(\"POSTGRES_HOST\")\n",
    "    DATABASE_USER = getenv(\"POSTGRES_USER\")\n",
    "    DATABASE_PASSWORD = getenv(\"POSTGRES_PASSWORD\")\n",
    "    DATABASE_NAME = getenv(\"POSTGRES_DB\")\n",
    "    DATABASE_PORT = getenv('POSTGRES_PORT')\n",
    "    all_variables = [DATABASE_HOST,\n",
    "                     DATABASE_USER,\n",
    "                     DATABASE_NAME,\n",
    "                     DATABASE_PASSWORD,\n",
    "                     DATABASE_PORT]\n",
    "    if all(all_variables):\n",
    "        return dict(zip(['database_host',\n",
    "                         'database_user',\n",
    "                         'database_name',\n",
    "                         'database_password',\n",
    "                         'database_port'], all_variables))\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Please add a .env file and put the credentials on it,\\\n",
    "                         refer to the sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_session(credentials):\n",
    "    \"\"\"\n",
    "    Create a database session for database task\n",
    "\n",
    "    Args:\n",
    "        credentials (dict): credentials to use to connect to the db\n",
    "\n",
    "    Returns:\n",
    "        [tuple]: database session and the engine\n",
    "    \"\"\"\n",
    "    database_url = 'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}'\\\n",
    "        .format(user=credentials.get('database_user'),\n",
    "                password=quote(credentials.get('database_password')),\n",
    "                host=credentials.get('database_host'),\n",
    "                database=credentials.get('database_name'),\n",
    "                port=credentials.get('database_port'))\n",
    "    engine = create_engine(database_url)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    return session, engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = read_credentails('.env')\n",
    "session, engine = get_database_session(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"select text, created_at, raw_json from tweet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with engine.connect() as connection:\n",
    "    data = pd.read_sql_query(sql=sql_query, con=connection, parse_dates=['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>raw_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>size gdp economier member stater east african ...</td>\n",
       "      <td>2023-02-21 14:04:56</td>\n",
       "      <td>{'text': 'RT @TanzaniaInsight: The size of GDP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>execution convention apparent colonisation eco...</td>\n",
       "      <td>2023-02-21 14:04:49</td>\n",
       "      <td>{'text': 'RT @StanysBujakera: #RDC: \"l'exécuti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2023-02-21 14:04:42</td>\n",
       "      <td>{'text': 'ハンドン様犬様と戯れてる', 'created_at': 1676988...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>today addi heads stater eac icglr reaffirmed s...</td>\n",
       "      <td>2023-02-21 14:04:40</td>\n",
       "      <td>{'text': 'RT @AmbMonicaJuma: 1/ Today in Addis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2023-02-21 14:04:33</td>\n",
       "      <td>{'text': '@sugiramireille @fernanddev @UNHuman...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          created_at  \\\n",
       "0  size gdp economier member stater east african ... 2023-02-21 14:04:56   \n",
       "1  execution convention apparent colonisation eco... 2023-02-21 14:04:49   \n",
       "2                                                    2023-02-21 14:04:42   \n",
       "3  today addi heads stater eac icglr reaffirmed s... 2023-02-21 14:04:40   \n",
       "4                                                    2023-02-21 14:04:33   \n",
       "\n",
       "                                            raw_json  \n",
       "0  {'text': 'RT @TanzaniaInsight: The size of GDP...  \n",
       "1  {'text': 'RT @StanysBujakera: #RDC: \"l'exécuti...  \n",
       "2  {'text': 'ハンドン様犬様と戯れてる', 'created_at': 1676988...  \n",
       "3  {'text': 'RT @AmbMonicaJuma: 1/ Today in Addis...  \n",
       "4  {'text': '@sugiramireille @fernanddev @UNHuman...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2658300, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet('data/tweets.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the table that we have all the tweets test and the time they where created, we those information we can go ahead and create our topic modelling modelt."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words representations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all know that computer process information which is only in binary or number format , but in most of the nlp task we are dealing with text format.\n",
    "This led to the question about the best way to store data for nlp task.? \n",
    "The first option that comes to our mind could be using one hot vector , each word can be encoded in a vector which has the size equals to the number of words in our vocabulary. This approach can work in example where we have a small vocabulary but for large vocabular it can lead to many issue since vector will grow larger and become sparse.\n",
    "Fortunately we have others approaches we can uses , in this sections we will explore the most commons ones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sections we will try differents words representation for our tweets, the most commons are: \n",
    "    - Bag of words \n",
    "    - Bag of n-grams : here we will try bigram and tri-grams\n",
    "    - TF-IDF stands for term frequency times inverse document frequency.\n",
    "    Term frequencies are the counts of each word in a document, which you\n",
    "    learned about in previous chapters. Inverse document frequency means that\n",
    "    you’ll divide each of those word counts\n",
    "    - PMI pointwise mutual information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let start with bag of words:\n",
    "\n",
    "_what are bag of words_?\n",
    "\n",
    "A bag word is a document representation were each words is represented with it frequency of occurence in a document or the number of times it occurs in a given document.\n",
    "\n",
    "By document we denote a collection of words it can be a tweets, a blog post or a stackoverflow question.\n",
    "\n",
    "Praticaly we use python counter to generate a dictionary of back of words with words and their frequency.\n",
    "\n",
    "This representation has some drawbacks which are : \n",
    "\n",
    "- we new words and not comons words the lengths of our vectors will increase\n",
    "- it can also lead to sparse vectors when we have rare words in a documents \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia define the term frequency tf as a numerical statistics that intent to reflect how important a wort is in to document in a collection or a corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$TF_{w, d} = \\frac{n_{w,d}}{number of term in the document}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the numerator of the above formula we have the number of times a words appear in a document d and the denominator contains the number of words in a document."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inverse document frequency "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse document frequency is them measure of how important a words is in a corpus.\n",
    "\n",
    "It is the log of the ratio between the total number of a documents in the corpus and the number of documents with the given word.\n",
    "\n",
    "$Idf_t = \\frac{nunber of documents }{nunber of document with the word w}$\n",
    "\n",
    "This helps to penalise the most commons in a corpus those words carry less values for our analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Term frequency inverse document frequency is the combinaison of both metrics , it helps to get how important a word is in a corpus and how impostant it is in a document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(tf_idf)_{t,d } =  Idf_t * TF_{w, d}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pratically let get the inverse document frequency matrix for our corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tweets(data, cleanned=True):\n",
    "    '''\n",
    "    this functions take the tweets data as parmeter \n",
    "    and return the vectorizeed version of the tweets\n",
    "    '''\n",
    "    tf_idf_vectorizer = TfidfVectorizer()\n",
    "    if cleanned:\n",
    "        data_tf_idf = tf_idf_vectorizer.fit_transform(data.cleanned_text)\n",
    "        return data_tf_idf, tf_idf_vectorizer\n",
    "    data_tf_idf = tf_idf_vectorizer.fit_transform(data.text)\n",
    "    return data_tf_idf, tf_idf_vectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see from this theat we have a large vocabulary with arround 215k words , this is huges and show that we may need some preprocessing in order to reduce the number of words in our corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve word lematization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this step I will be using lematization instead of stemming, \n",
    "Lemmatization is define as : \n",
    "\n",
    "_Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw_ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it will basically help to replace the words with their parent words in the base language :\n",
    "    - the nouns nous `voulons` will mean `vouloir`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach has some advantages because it will help to not spread the information between different words forms derived from the same lemma, therefore it will lead to an accurate topic modelling because the same semantic information is assembled in one place."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also have some disadvantages : \n",
    "new ambiguities may arise the words such as “je suis” should be rendered as “suivre” (to follow) or “être” (to be)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let now try to see it in pratices how it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematize_text(text):\n",
    "    doc = nlp(text)\n",
    "    cleanned_text = \" \".join([token.lemma_ for token in doc])\n",
    "    return cleanned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bf639c4f2b4695b00b02678ccb0b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='elapsed time when applying the function'), FloatProgress(value=0.0, max=1214425.0),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data['cleanned_text'] = data.text.swifter.progress_bar(enable=True, desc='elapsed time when applying the function').apply(lematize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>cleanned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1260628656327725056</td>\n",
       "      <td>zambie militaire zambien occuper bout territoi...</td>\n",
       "      <td>2020-05-13 17:51:07</td>\n",
       "      <td>zambie militaire zambien occuper bout territoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1260628636710961153</td>\n",
       "      <td>covid essai clinique remède curatif bientôt dé...</td>\n",
       "      <td>2020-05-13 17:51:02</td>\n",
       "      <td>covid essai clinique remède curatif bientôt dé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1260628592289030144</td>\n",
       "      <td>poursuivre justice .. val mieux contract gre g...</td>\n",
       "      <td>2020-05-13 17:50:52</td>\n",
       "      <td>poursuivre justice .. val mieux contract gre g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1260628553504305153</td>\n",
       "      <td>jemapeleeeeeee madameeeee camp fcc bokola kooo...</td>\n",
       "      <td>2020-05-13 17:50:43</td>\n",
       "      <td>jemapeleeeeeee madameeee camp fcc bokola koooo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1260628552426369024</td>\n",
       "      <td>fin poursuite judiciaire procès onem accueilli...</td>\n",
       "      <td>2020-05-13 17:50:42</td>\n",
       "      <td>fin poursuite judiciaire procès onem accueilli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1260628656327725056  zambie militaire zambien occuper bout territoi...   \n",
       "1  1260628636710961153  covid essai clinique remède curatif bientôt dé...   \n",
       "2  1260628592289030144  poursuivre justice .. val mieux contract gre g...   \n",
       "3  1260628553504305153  jemapeleeeeeee madameeeee camp fcc bokola kooo...   \n",
       "4  1260628552426369024  fin poursuite judiciaire procès onem accueilli...   \n",
       "\n",
       "           created_at                                      cleanned_text  \n",
       "0 2020-05-13 17:51:07  zambie militaire zambien occuper bout territoi...  \n",
       "1 2020-05-13 17:51:02  covid essai clinique remède curatif bientôt dé...  \n",
       "2 2020-05-13 17:50:52  poursuivre justice .. val mieux contract gre g...  \n",
       "3 2020-05-13 17:50:43  jemapeleeeeeee madameeee camp fcc bokola koooo...  \n",
       "4 2020-05-13 17:50:42  fin poursuite judiciaire procès onem accueilli...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with new word lemmatization we can improve the pre processing step of those tweets and try to improve our model in productions<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/cleanned_tweets_2021.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us try the tf idf vectorizer apain to examine how many vector we got \n",
    "\n",
    "https://dragonfly.hypotheses.org/648\n",
    "\n",
    "https://stats.stackexchange.com/questions/179349/what-are-the-pros-and-cons-of-applying-pointwise-mutual-information-on-a-word-co\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = vectorize_tweets(data, cleanned=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set()\n",
    "for sentence in data.text:\n",
    "    for word in sentence.split(' '):\n",
    "        word_set.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanned_word_set = set()\n",
    "for sentence in data.cleanned_text:\n",
    "    for word in sentence.split(' '):\n",
    "        cleanned_word_set.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4098"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set) - len(cleanned_word_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the cleanning process I gained somethilg arroud 4k words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improve tokenization ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can  also improve the preprocessing using a custom made tokenizer for social media data.\n",
    "\n",
    "The NLTK library includes a tokenizer—casual_tokenize—that was built to deal\n",
    "with short, informal, emoticon-laced texts from social networks where grammar and\n",
    "spelling conventions vary widely.\n",
    "The casual_tokenize function allows you to strip\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source : the nltk book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be incorporate in the global text preprocessing pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N gram before topic modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data, vectorizer = vectorize_tweets(data, cleanned=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still having a lot of word which are not written in the correct format and we may need to clean them using different cleanning techmics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first idea that comes to my mind is to use count vectorizer to count the most frequent word in the corpus and from that we can see if the misspelled word are frequent in the top words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I can also do some clustering using the leveinsten distance to see which word are similars to each other using the leveinsten distance and then group them in clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I can also learn embedding from my word and check if the 2 version of the same words with almost the same spelling are similar using the cosine distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- or I can learn the enmbedding but use cosine distance and the leveisten distance as similarity measure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the cleanning we can see that the len of the word have reduced in ouw vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'100jour',\n",
    " '100jours',\n",
    " '100jpe',\n",
    " '100jr',\n",
    " '100jrs',"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'28sept',\n",
    " '28septembre',\n",
    " '28settembr',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1214425, 210907)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1214425x210907 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8289054 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a comment I got  from a friend who is  NLP researchses  he said that that extra preprocessing step is not important for our project , we can start by building it from that and see how our result will behave and how the model perfom on differents evaluations metrics . From that we can come back on the cleanning process "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
