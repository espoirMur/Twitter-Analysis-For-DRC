{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from pathlib import Path\n",
    "from urllib.parse import quote \n",
    "\n",
    "def get_dot_env_file(ENV):\n",
    "    if ENV:\n",
    "        return Path.cwd().joinpath(ENV)\n",
    "    return find_dotenv(Path.cwd().parent.joinpath(\".env.dev\"))\n",
    "\n",
    "def read_credentails(file='.env'):\n",
    "    \"\"\"\n",
    "    Return users credentials from the environnement variable\n",
    "    raise a an exception if the credentials are empty\n",
    "\n",
    "    Raises:\n",
    "        ValueError: raise a value error if no credentials was found\n",
    "    \"\"\"\n",
    "    dot_env_file = get_dot_env_file(file)\n",
    "    load_dotenv(dotenv_path=dot_env_file, override=True)\n",
    "    DATABASE_HOST = getenv(\"POSTGRES_HOST\")\n",
    "    print(f\"the database host is {DATABASE_HOST}\", 10 * \"*==-\")\n",
    "    DATABASE_USER = getenv(\"POSTGRES_USER\")\n",
    "    DATABASE_PASSWORD = getenv(\"POSTGRES_PASSWORD\")\n",
    "    DATABASE_NAME = getenv(\"POSTGRES_DB\")\n",
    "    DATABASE_PORT = getenv('POSTGRES_PORT')\n",
    "    all_variables = [DATABASE_HOST,\n",
    "                     DATABASE_USER,\n",
    "                     DATABASE_NAME,\n",
    "                     DATABASE_PASSWORD,\n",
    "                     DATABASE_PORT]\n",
    "    if all(all_variables):\n",
    "        return dict(zip(['database_host',\n",
    "                         'database_user',\n",
    "                         'database_name',\n",
    "                         'database_password',\n",
    "                         'database_port'], all_variables))\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Please add a .env file and put the credentials on it,\\\n",
    "                         refer to the sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_session(credentials):\n",
    "    \"\"\"\n",
    "    Create a database session for database task\n",
    "\n",
    "    Args:\n",
    "        credentials (dict): credentials to use to connect to the db\n",
    "\n",
    "    Returns:\n",
    "        [tuple]: database session and the engine\n",
    "    \"\"\"\n",
    "    database_url = 'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}'\\\n",
    "        .format(user=credentials.get('database_user'),\n",
    "                password=quote(credentials.get('database_password')),\n",
    "                host=credentials.get('database_host'),\n",
    "                database=credentials.get('database_name'),\n",
    "                port=credentials.get('database_port'))\n",
    "    print(database_url, \"==\"* 5)\n",
    "    engine = create_engine(database_url)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    return session, engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the database host is 89.40.12.167 *==-*==-*==-*==-*==-*==-*==-*==-*==-*==-\n",
      "{'database_host': '89.40.12.167', 'database_user': 'es_py', 'database_name': 'tweets_analysis', 'database_password': '9874@Postgres', 'database_port': '5432'}\n",
      "postgresql+psycopg2://es_py:9874%40Postgres@89.40.12.167:5432/tweets_analysis ==========\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "current_dir = Path().cwd()\n",
    "credentials = read_credentails('.env')\n",
    "print(credentials)\n",
    "session, engine = get_database_session(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this part of the code is duplicated in the main file , i need to create a module for it ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "sql_query = f\"select  raw_json->>'text' as tweet_text from tweet where date(created_at) = '{today}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"select  raw_json->>'text' from tweet where date(created_at) = '2021-11-25'\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = None\n",
    "with engine.connect() as connection:\n",
    "    data = pd.read_sql_query(sql=sql_query, con=connection, parse_dates=['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coordinateur·trice Santé - RDC - Kinshasa\\nhtt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@BleacherReport @KingJames Still the best on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@BenitaNtumba @fatshi13 @JacquesKyabula @kabun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@NALAmoney @Benji_Fernandes Nala to DRC before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@InterCronicas coragem kkkkk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text\n",
       "0  Coordinateur·trice Santé - RDC - Kinshasa\\nhtt...\n",
       "1  @BleacherReport @KingJames Still the best on t...\n",
       "2  @BenitaNtumba @fatshi13 @JacquesKyabula @kabun...\n",
       "3  @NALAmoney @Benji_Fernandes Nala to DRC before...\n",
       "4                       @InterCronicas coragem kkkkk"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleaning  the text\n",
    "\n",
    "in the part we will be performing the text cleaning using our deflaut text cleanner and using an new tweet tokenizer from huggingfacem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text    @BenitaNtumba @fatshi13 @JacquesKyabula @kabun...\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@Bleacherreport @Kingjames Still The Best On The Planet',\n",
       "       '@Intercronicas Coragem Kkkkk',\n",
       "       '@Kelly_Rdc Https://T.Co/N7Gbedqa7P', ...,\n",
       "       '@Fernandoaquezad @Kelly_Rdc Ahhh... I Was Gonna Say. I Know Times Have Changed, But I Still Dig The Chicks, Bro.\\n\\nI… Https://T.Co/T5Mkmpvrm6',\n",
       "       '@Innocentmirimo3 @Fatshi13 Kagame Se Sent Vraiment Chez Lui Après Le Départ De Jkk. Quelle Image Fatshi Va Laisser… Https://T.Co/Qms0Lkvh69',\n",
       "       'My Father Is A Retired Soldier, His Last Mission Was In Drc, He Fought In The Liberation War, So Many Stories About… Https://T.Co/Dbtdjs1Tde'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweets_cleaner.TweetsCleaner import TweetsCleaner\n",
    "cleaner = TweetsCleaner('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_tweet'] = data['tweet_text'].apply(cleaner.prepocess_tweet, args=(True, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to explore for lemmatizaion\n",
    "\n",
    "https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'president felix antoine'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_accents(\"président felix antoine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13083    [felix, tshisekedi, accueille, chef, detat, je...\n",
       "1678               [coffee, family, seeing, god, one, day]\n",
       "8457     [remember, rumors, kagame, death, whole, popul...\n",
       "3709     [nadie, dice, las, patovacacione, desacostumbr...\n",
       "15158    [acho, minimo, poderia, fazer, depois, desser,...\n",
       "2039     [shared, people, mak, initial, enquiry, text, ...\n",
       "1099     [congratulation, raymond, who, key, turkey, bo...\n",
       "3277                                                    []\n",
       "3467     [echange, depolitisation, ndoto, baba, solutio...\n",
       "13135              [cher, compatriote, president, mausole]\n",
       "Name: cleaned_tweet, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_tweet'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can stay that a bit of the text cleaning part is done, but now we need to improve the lematizaion part, specially for nouns, for example Kinshasa is lematized as Kinshaser, not Kinshasa city. Checkout this [link](https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer) to see how the lematization is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let now move to the sentiment analysis part of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis\n",
    "\n",
    "\n",
    "This is a case for sentiment analysis but we will not try to do it in a supervised way, we will try to do it in an unsupervised way. using the [Vader library](https://github.com/cjhutto/vaderSentiment) for french sentiment analysis.\n",
    "\n",
    "For supervised sentiment analysis it worth exploring the BERT tool here [here](https://github.com/TheophileBlard/french-sentiment-analysis-with-bert) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment_fr.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analyzer.polarity_scores(' '.join(data['cleaned_tweet'][3467]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4deafcc533acb4af87dfa9818a4849fc8a3ecd558ef37d2eef483ad9d23de92"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
