{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"cite2c-biblio\"></div><div class=\"cite2c-biblio\"></div># Topic Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we wiil texplain what we first understand by topic modelling, what are the most used methods for topic modelling and I will conclude with dynamic topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In standard topic modelling we assume the order of document does not matter and therfore this approach is not suitable for time-stamped data. \n",
    "Dynamic topic modelling on the other end approach to track how languages and topic changes in a time-stamped corpus.\n",
    "\n",
    "_cite this paper from Derek Greene here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach for the topic modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposed approach is a two level one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first step is to divide the corpus into n time windows of equals durations for example days, hours, and any other time windows we want "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On the first we apply the topic modelling techic to the document in each time windows to produce windows topics. As per the topic modelling definition the result of this step generate n windowfs topics matrix and each windows we have 2 differents matrix one is the document term matrix $W$ and the other one is the topic term matrix $H$.\n",
    "\n",
    "\n",
    "- On the second  level  we apply another MNF to all topic from step one to find dynamic topic which span multiple times windows. In this step we bacically take the most important topic in each $H$ matrix form step one and stack them row wise to have another topic per time windows and term matrix which we consider as an input matrix after another NMF decomposition of this matrix we have the time topic matrix and another topic term matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynaminc algorithm can be represented in the fololwing steps : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we start with an empty matrix $B$ \n",
    "- for each windows topic model $M_t$: \n",
    "    - for each row in the $M_t$ matrix select $n$ top - ranked topic term and fill the other row with zeroes\n",
    "    - add the vector as a new vector of $B$\n",
    "- Once all the row have been added removed all the colums with zeros everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $B$ resulted from this operation is an $i$ x $m'$ matrix where  i is the number of all the topic documents and m' is the number of most used terms in those topics . from the last step of our algo we can see that $m' <<< m $ with this we can see that we have removed all the terms which are not most used in our timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of the B matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all know that the row of the $B$ matrix are the time windows topic and the columns are the top terms in each time windows topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the new $B$ matrix we can now apply another $NMF$  factorization to get the evolution of the dynamic topic over the time. \n",
    "\n",
    "He factorization will of B will yield the following result $B = UV$ , in this decompozition we have 2 matrixes $U$ and $V$.\n",
    "\n",
    "What do we have in those matrixes ? \n",
    "\n",
    "$U$ : the row of this matrix are the time windows topic, and the colums we have the weight of the of each dynamic topic.\n",
    "\n",
    "$V$ : the values in the row are the dynamic topic themselves and the colums are the terms describing the dynamic topics and their weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the weight of the dynamics topic in $U$ to track the evolution of topic over time in our document we can find when it appear and when it when it disapear in the corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let move to the application of those methods and see what result we will get ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coding the dynamic topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook about data cleanning we saved our cleanned dataset under `../data/cleanned_tweets_2021.csv`\n",
    "\n",
    "Here we will just load it and apply a tfid vectorizer to it to get our documend term matrix that we will split into the different time windows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/es.py/Projects/Personal/nlp_course_coursera_russia_uni/.venv/lib/python3.6/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/es.py/Projects/Personal/nlp_course_coursera_russia_uni/.venv/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/cleanned_tweets_2021.csv', index_col='id', parse_dates=['created_at'])[['created_at', 'cleanned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step one split the data into time windows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = data.created_at.min().strftime('%d-%b-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = data.created_at.max().strftime('%d-%b-%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "max_date": "19-Dec-2020",
     "min_date": "13-May-2020"
    }
   },
   "source": [
    "From this we can see that our data where collected from {{min_date}} to {{max_date}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us now split it into differend time windows of different using the week as the time measurement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_groups = data.groupby(pd.Grouper(key=\"created_at\", freq=\"1W\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_data = [df for time, df in windows_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(windows_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>cleanned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1315442075488473093</th>\n",
       "      <td>2020-10-12 00:00:05</td>\n",
       "      <td>fall offcolored kirby get digitalreservetoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315442081998004224</th>\n",
       "      <td>2020-10-12 00:00:06</td>\n",
       "      <td>vouloir venir culpabiliser traiter xénophobe p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315442083612819456</th>\n",
       "      <td>2020-10-12 00:00:07</td>\n",
       "      <td>cinéma premier projection film lagence présenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315442148964274176</th>\n",
       "      <td>2020-10-12 00:00:22</td>\n",
       "      <td>tête rien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315442210092048384</th>\n",
       "      <td>2020-10-12 00:00:37</td>\n",
       "      <td>learn mna racial diversity committ statement s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317978565154119680</th>\n",
       "      <td>2020-10-18 23:59:11</td>\n",
       "      <td>manooo evento amo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317978669252489216</th>\n",
       "      <td>2020-10-18 23:59:36</td>\n",
       "      <td>started going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317978686210048006</th>\n",
       "      <td>2020-10-18 23:59:40</td>\n",
       "      <td>bruno miteyo nyeng nouveau chef maison civil c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317978695055908864</th>\n",
       "      <td>2020-10-18 23:59:42</td>\n",
       "      <td>nigerier siler holocaust happening one talking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317978737443524608</th>\n",
       "      <td>2020-10-18 23:59:52</td>\n",
       "      <td>nigerier siler holocaust happening one talking...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170758 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             created_at  \\\n",
       "id                                        \n",
       "1315442075488473093 2020-10-12 00:00:05   \n",
       "1315442081998004224 2020-10-12 00:00:06   \n",
       "1315442083612819456 2020-10-12 00:00:07   \n",
       "1315442148964274176 2020-10-12 00:00:22   \n",
       "1315442210092048384 2020-10-12 00:00:37   \n",
       "...                                 ...   \n",
       "1317978565154119680 2020-10-18 23:59:11   \n",
       "1317978669252489216 2020-10-18 23:59:36   \n",
       "1317978686210048006 2020-10-18 23:59:40   \n",
       "1317978695055908864 2020-10-18 23:59:42   \n",
       "1317978737443524608 2020-10-18 23:59:52   \n",
       "\n",
       "                                                         cleanned_text  \n",
       "id                                                                      \n",
       "1315442075488473093      fall offcolored kirby get digitalreservetoken  \n",
       "1315442081998004224  vouloir venir culpabiliser traiter xénophobe p...  \n",
       "1315442083612819456  cinéma premier projection film lagence présenc...  \n",
       "1315442148964274176                                          tête rien  \n",
       "1315442210092048384  learn mna racial diversity committ statement s...  \n",
       "...                                                                ...  \n",
       "1317978565154119680                                  manooo evento amo  \n",
       "1317978669252489216                                      started going  \n",
       "1317978686210048006  bruno miteyo nyeng nouveau chef maison civil c...  \n",
       "1317978695055908864  nigerier siler holocaust happening one talking...  \n",
       "1317978737443524608  nigerier siler holocaust happening one talking...  \n",
       "\n",
       "[170758 rows x 2 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows_data[-10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "len(windows_data)": "32"
    }
   },
   "source": [
    "After splitting we can find that our dataset is splited in time windows of {{len(windows_data)}} weeks , with the splitting done we can continue with the topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay what is next is to start analyzing the topic in each time windows and build a pipeline to cobine the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should I build the vectorizer for all the topic or for each time windows I need to build  a tokenizer ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dereek suggest to apply the preproccessing for each time windows , we are going to build the documend term matrix for every timewindows and return the document term matrix for each time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, \n",
    "                             strip_accents=\"unicode\", \n",
    "                             ngram_range = (1, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>cleanned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [created_at, cleanned_text]\n",
       "Index: []"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_data = list(filter( lambda x: not x.empty, windows_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(windows_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'présidence cabinet chef cabinet conseiller spécial chef etat matir'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows_data[2].head(49).iloc[1].cleanned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_term_matrix = vectorizer.fit_transform(windows_data[0].cleanned_text.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14561, 53641)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "doc_term_matrix.shape[0]": "14561",
     "doc_term_matrix.shape[1]": "53641"
    }
   },
   "source": [
    "we have the for the first time windows the a tdifdf matrix of {{doc_term_matrix.shape[0]}} rows and {{doc_term_matrix.shape[1]}} colunms, those columns are the number of words and bi_grams we have in our corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do futher preprocessing by looking into the most common ngrams we have in our corpus by using the following function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "def rank_terms( doc_term_matrix, terms ):\n",
    "    # get the sums over each column\n",
    "    sums = doc_term_matrix.sum(axis=0)\n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    for col, term in enumerate(terms):\n",
    "        weights[term] = sums[0,col]\n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thread', 218.83351691982568),\n",
       " ('zair', 217.28855207461424),\n",
       " ('anecdote', 214.5299626363258),\n",
       " ('anecdote zair', 214.5299626363258),\n",
       " ('thread anecdote', 214.5299626363258),\n",
       " ('covid', 96.90122318562476),\n",
       " ('mai', 91.11460570791263),\n",
       " ('pays', 81.31555017712598),\n",
       " ('jour', 72.42982024090871),\n",
       " ('ministre', 61.22501661962492),\n",
       " ('cas', 60.341136153752345),\n",
       " ('tshisekedi', 59.57607977234457),\n",
       " ('kabila', 56.10635901834737),\n",
       " ('kinshasa', 55.67061650966388),\n",
       " ('general', 54.4249999633158),\n",
       " ('president', 54.33813823016058),\n",
       " ('dire', 48.45230863613515),\n",
       " ('felix', 47.58595614377553),\n",
       " ('national', 46.989097460364775),\n",
       " ('demande', 46.66212030084455),\n",
       " ('passeport', 46.48247484132089),\n",
       " ('arreter', 46.340508559153186),\n",
       " ('felix tshisekedi', 45.19220772334301),\n",
       " ('justice', 44.06933342729221),\n",
       " ('creer', 43.74798354294154),\n",
       " ('pouvoir', 43.652306778435666),\n",
       " ('passer', 41.289866489392864),\n",
       " ('nouveau', 40.89890882983724),\n",
       " ('monde', 40.14428955122549),\n",
       " ('jeune', 39.82796088655963)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_terms(doc_term_matrix, terms)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the twenty most recent terms in the first time windows , let us know apply the topic modelling to it to see how what will result from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "k": "10"
    }
   },
   "source": [
    "let us start with a k = {{k}} and then we will incorporate modele selection and performance measurenment metrics later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non svd is recomend to because it provide more reliable results (cite: https://www.sciencedirect.com/science/article/abs/pii/S0031320307004359?via%3Dihub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=k, init=\"nndsvd\", random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = model.fit_transform(doc_term_matrix)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14561, 10)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.014, 0.017,\n",
       "       0.   ])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[3,:].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 53641)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H contains the term weight for all topic and in from that we can pick one term and check how it is weigthed in each topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.   , 0.006, 0.004, 0.   , 0.013, 0.003, 0.   , 0.009,\n",
       "       0.002])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_index = terms.index('passeport')\n",
    "H[:, term_index].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this we can find that the term passeport is highly linked with approximately 3 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see if now our topic make senses , we will use a functin called topic descriptor , for each topic it will return the top most ranked term in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptor( terms, H, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( (terms[term_index], H[topic_index, term_index].round(2)) )\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1.0 === descripe by : thread=:2.098826947152738, anecdote zair=:2.09821705004985, thread anecdote=:2.09821705004985, anecdote=:2.09821705004985, zair=:2.062225949589958, symboliser envahissement=:0.005933999451435938, malheur boot=:0.005933999451435938, symboliser=:0.005933999451435938, boot=:0.005933999451435938, jour symboliser=:0.005933999451435938\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "Topic 2.0 === descripe by : world shortest=:0.7977587288636605, shortest set=:0.7977587288636605, leav villag=:0.7977587288636605, set people=:0.7977587288636605, african pygmie=:0.7977587288636605, people friendly=:0.7977587288636605, pygmie world=:0.7977587288636605, never leav=:0.7977587288636605, friendly=:0.7977587288636605, shortest=:0.7977587288636605\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "Topic 3.0 === descripe by : rappeur damso=:0.8278004304520612, fondation afin=:0.8278004304520612, vol ressource=:0.8278004304520612, damso=:0.8278004304520612, afin lutter=:0.8278004304520612, damso creer=:0.8278004304520612, lutter vol=:0.8278004304520612, creer fondation=:0.8278004304520612, rappeur=:0.8278004304520612, ressource naturel=:0.8266429424185342\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "Topic 4.0 === descripe by : felix tshisekedi=:0.7617541675627449, felix=:0.7546356791774452, dossier jour=:0.7508989277230919, audit inspection=:0.7378005096379111, financ ministre=:0.7378005096379111, inspection=:0.7378005096379111, inspection general=:0.7378005096379111, jour felix=:0.7378005096379111, demande audit=:0.7378005096379111, tshisekedi demande=:0.7378005096379111\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "Topic 5.0 === descripe by : composer cuivre=:0.6687145782358357, richess composer=:0.6687145782358357, cuivre cobalt=:0.6687145782358357, cuivre=:0.6687145782358357, soussol=:0.6687145782358357, soussol richess=:0.6687145782358357, zinc fer=:0.6687145782358357, richesse soussol=:0.6687145782358357, zinc=:0.6687145782358357, cobalt zinc=:0.6687145782358357\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "Topic 6.0 === descripe by : election miss=:0.6598085154654332, europe derouler=:0.6598085154654332, passer france=:0.6598085154654332, miss europe=:0.6598085154654332, revolte=:0.6598085154654332, kinshasa lieu=:0.6598085154654332, cause revolte=:0.6598085154654332, france cause=:0.6598085154654332, derouler kinshasa=:0.6598085154654332, derouler=:0.6598085154654332\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "Topic 7.0 === descripe by : enquete judiciaire=:0.6899117871141792, dilatoire=:0.678844448282219, jouer dilatoire=:0.678844448282219, dilatoire enquete=:0.678844448282219, soupconner=:0.6754756916044793, soupconner ministre=:0.6733680510463368, acaj soupconner=:0.6733680510463368, justice jouer=:0.6733680510463368, enquete=:0.6730547110886055, judiciaire=:0.6726541871983476\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "Topic 8.0 === descripe by : pari=:0.5985193019335951, france atteri=:0.5949389175146651, quarantain pouvonsnou=:0.5949389175146651, atteri hier=:0.5949389175146651, pouvonsnou=:0.5949389175146651, hier provenance=:0.5949389175146651, atteri=:0.5949389175146651, pari mettre=:0.5949389175146651, mettre quarantain=:0.5949389175146651, passager avion=:0.5949389175146651\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "Topic 9.0 === descripe by : mai=:0.8960631996319879, mai jour=:0.8401386380736203, jour ferie=:0.77673438133537, constat mai=:0.77673438133537, souvent savoir=:0.77673438133537, pose souvent=:0.77673438133537, ferie=:0.77673438133537, ferie pose=:0.77673438133537, constat=:0.7710491103988304, jour=:0.7526101924098699\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "Topic 10.0 === descripe by : ministere=:1.5298866290324904, defense demande=:0.8357307754361398, obsequ=:0.8276999750216845, obsequ general=:0.8237724805106805, defense=:0.7973424226801763, demande ministere=:0.785499188627767, ministere financ=:0.7814445069651141, financ obsequ=:0.7741213018709862, ministere defense=:0.772153537439267, general=:0.7258593517570786\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n"
     ]
    }
   ],
   "source": [
    "descriptors = []\n",
    "for topic_index in range(k):\n",
    "    descriptor = get_descriptor( terms, H, topic_index, 10 ) \n",
    "    descriptor = [f\"{x[0]}=:{x[1]}\" for x in descriptor]\n",
    "    descriptor = ', '.join(descriptor)\n",
    "    current_topic = topic_index+1\n",
    "    print(f\"Topic {current_topic:.1f} === describe by : {descriptor}\")\n",
    "    print('=:'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a human evaluation percepective the topic and the description make sense we can now see how the. enmbedding train in our corpus make sense in for those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of our models and parameter selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this we will use coherence measures using embeddings to detects how good are our generated topics. \n",
    "\n",
    "what are those coherence measures? \n",
    "\n",
    "Here we will define the topice coherence which the sum of pair wise similaries of words that describes a topic? _why not weigthed it by topic weigth?_\n",
    "\n",
    "Basically it is define as \n",
    "\n",
    "\n",
    "For each topic we can do :\n",
    "\n",
    "$TC =  \\sum_{j=2}^{N}\\sum_{i=2}^{j-1}{similarity(w_i, w_j)}$\n",
    "\n",
    "\n",
    "For all the topics we define it as : \n",
    "\n",
    "it is the mean of TC for all the topic in the $H$ matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity measure we use is the cosine distance between the enmbeddings of words we have in our corpus. We can use any enmbedding techinc but here we suggest to learn embedding for our corpus since we have many term specific to the congolese context which are not in the well know enmbeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning words embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are planning to use pmi mutual information to train our word embeddings , and we evaluate the results and how they perform for mostly misspeled words if they are not performing well we will try to train a custom character level embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check [this](https://towardsdatascience.com/embedding-for-spelling-correction-92c93f835d79) to check how we can improve our embeddings to take into consideration misspled words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite the facebook paper where they build a model to check for misspled charactere\n",
    "\n",
    "This one : __*arXiv:1905.09755v1 [cs.CL] 23 May 2019*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continue wih this research to complete the embedding trainning \n",
    "\n",
    "https://www.kaggle.com/alexklibisz/simple-word-vectors-with-co-occurrence-pmi-and-svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are planning to use to train the charactere n-gram embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@article{bojanowski2017enriching,\n",
    "  title={Enriching Word Vectors with Subword Information},\n",
    "  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n",
    "  journal={Transactions of the Association for Computational Linguistics},\n",
    "  volume={5},\n",
    "  year={2017},\n",
    "  issn={2307-387X},\n",
    "  pages={135--146}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course-venv-v2",
   "language": "python",
   "name": "nlp-course-venv-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
